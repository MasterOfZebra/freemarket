"""
–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –ø–æ–¥–±–æ—Ä–∞ –æ–±–º–µ–Ω–æ–≤
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å–º—ã—Å–ª–∞ –∑–∞–ø—Ä–æ—Å–æ–≤, —Å–∏–Ω–æ–Ω–∏–º—ã, –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from backend.language_normalization import get_normalizer
    from backend.equivalence_engine import ExchangeEquivalence
    from backend.models import ListingItem, ExchangeType, ListingItemType
except ImportError:
    # Fallback for direct execution
    from language_normalization import get_normalizer
    from equivalence_engine import ExchangeEquivalence
    from models import ListingItem, ExchangeType, ListingItemType


def create_test_item(item_name, category, exchange_type, value_tenge, duration_days=None):
    """–°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π ListingItem"""
    return ListingItem(
        listing_id=1,  # dummy
        item_type=ListingItemType.WANT,  # dummy
        category=category,
        exchange_type=ExchangeType.PERMANENT if exchange_type == "permanent" else ExchangeType.TEMPORARY,
        item_name=item_name,
        value_tenge=value_tenge,
        duration_days=duration_days,
        description=""
    )


class MatchingComprehensionTester:
    """–¢–µ—Å—Ç–µ—Ä —Å–∏—Å—Ç–µ–º—ã –ø–æ–Ω–∏–º–∞–Ω–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""

    def __init__(self):
        self.normalizer = get_normalizer()
        self.equivalence_engine = ExchangeEquivalence()

    def test_semantic_accuracy(self):
        """1.1 –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å"""
        print("\n" + "="*80)
        print("üéØ 1.1 –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –¢–û–ß–ù–û–°–¢–¨")
        print("="*80)

        test_cases = [
            # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ (–¥–æ–ª–∂–Ω—ã –Ω–∞–π—Ç–∏—Å—å)
            ("–≤–µ–ª–æ—Å–∏–ø–µ–¥", "–±–∞–π–∫", "transport", "temporary", 30000, 30000, 7, 7, "‚úÖ –î–û–õ–ñ–ï–ù –ù–ê–ô–¢–ò: —Å–∏–Ω–æ–Ω–∏–º—ã"),
            ("–Ω–æ—É—Ç–±—É–∫ Apple", "MacBook", "electronics", "permanent", 600000, 600000, None, None, "‚úÖ –î–û–õ–ñ–ï–ù –ù–ê–ô–¢–ò: –±—Ä–µ–Ω–¥ —Å–∏–Ω–æ–Ω–∏–º"),
            ("—Ä–µ–º–æ–Ω—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–∞", "–ø–æ—á–∏–Ω–∫–∞ —Å–º–∞—Ä—Ç—Ñ–æ–Ω–∞", "services", "temporary", 5000, 5000, 1, 1, "‚úÖ –î–û–õ–ñ–ï–ù –ù–ê–ô–¢–ò: —Å–º—ã—Å–ª–æ–≤—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã"),

            # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ (–ù–ï –¥–æ–ª–∂–Ω—ã –Ω–∞–π—Ç–∏—Å—å)
            ("—Ä–µ–º–æ–Ω—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–∞", "–ø—Ä–æ–¥–∞–∂–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–∞", "services", "temporary", 5000, 500000, 1, None, "‚ùå –ù–ï –î–û–õ–ñ–ï–ù –ù–ê–ô–¢–ò: —Ä–∞–∑–Ω—ã–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è"),
            ("–ø—Ä–æ–∫–∞—Ç –≤–µ–ª–æ—Å–∏–ø–µ–¥–∞", "–ø–æ–∫—É–ø–∫–∞ –≤–µ–ª–æ—Å–∏–ø–µ–¥–∞", "transport", "temporary", 1000, 50000, 7, None, "‚ùå –ù–ï –î–û–õ–ñ–ï–ù –ù–ê–ô–¢–ò: –∞—Ä–µ–Ω–¥–∞ vs –ø–æ–∫—É–ø–∫–∞"),
        ]

        results = []
        for want_text, offer_text, category, ex_type, want_price, offer_price, want_days, offer_days, expected in test_cases:
            print(f"\nüìã –¢–µ—Å—Ç: '{want_text}' ‚Üî '{offer_text}'")

            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ items
            want_item = create_test_item(want_text, category, ex_type, want_price, want_days)
            offer_item = create_test_item(offer_text, category, ex_type, offer_price, offer_days)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º language similarity
            lang_score = self.normalizer.similarity_score(want_text, offer_text)
            print(f"  Language similarity: {lang_score:.3f}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º equivalence
            if ex_type == "permanent":
                equiv_result = self.equivalence_engine.calculate_permanent_score(want_price, offer_price)
            else:
                equiv_result = self.equivalence_engine.calculate_temporary_score(
                    want_price, want_days or 1, offer_price, offer_days or 1
                )

            print(f"  Equivalence score: {equiv_result.score:.3f} ({equiv_result.category.value})")

            # Combined score (70% equivalence + 30% language)
            combined_score = equiv_result.score * 0.7 + lang_score * 0.3
            print(f"  Combined score: {combined_score:.3f}")

            # –û—Ü–µ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            is_match = equiv_result.is_match and combined_score >= 0.70
            should_match = "‚úÖ –î–û–õ–ñ–ï–ù" in expected

            if should_match and is_match:
                status = "‚úÖ PASS"
            elif not should_match and not is_match:
                status = "‚úÖ PASS"
            else:
                status = "‚ùå FAIL"

            print(f"  Result: {status} | Expected: {expected}")
            print(f"  Match: {is_match} | Combined: {combined_score:.3f}")

            results.append({
                'test': f"{want_text} ‚Üî {offer_text}",
                'lang_score': lang_score,
                'equiv_score': equiv_result.score,
                'combined_score': combined_score,
                'is_match': is_match,
                'expected_match': should_match,
                'status': status
            })

        # –ò—Ç–æ–≥–∏
        passed = sum(1 for r in results if r['status'] == "‚úÖ PASS")
        total = len(results)
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏: {passed}/{total} ({passed/total*100:.1f}%)")

        return results

    def test_grammar_orthography(self):
        """1.2 –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ –∏ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—è"""
        print("\n" + "="*80)
        print("üìù 1.2 –ì–†–ê–ú–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –í–ê–†–ò–ê–¶–ò–ò –ò –û–†–§–û–ì–†–ê–§–ò–Ø")
        print("="*80)

        test_cases = [
            # –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ (–¥–æ–ª–∂–Ω—ã –Ω–∞–π—Ç–∏—Å—å)
            ("–∏–≥—Ä–æ–≤–∞—è –ø—Ä–∏—Å—Ç–∞–≤–∫–∞", "–∏–≥—Ä–æ–≤—ã–µ –ø—Ä–∏—Å—Ç–∞–≤–∫–∏", "electronics", "permanent", 50000, 50000, "‚úÖ –î–û–õ–ñ–ï–ù –ù–ê–ô–¢–ò: —Ñ–æ—Ä–º—ã —á–∏—Å–ª–∞"),
            ("–≤–µ–ª–æ—Å–µ–ø–µ–¥", "–≤–µ–ª–æ—Å–∏–ø–µ–¥", "transport", "temporary", 30000, 30000, 7, 7, "‚úÖ –î–û–õ–ñ–ï–ù –ù–ê–ô–¢–ò: –æ–ø–µ—á–∞—Ç–∫–∞"),
            ("—Å–¥–∞—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É", "—Å–¥–∞—é –∫–≤–∞—Ä—Ç–∏—Ä—É", "housing", "temporary", 15000, 15000, 30, 30, "‚úÖ –î–û–õ–ñ–ï–ù –ù–ê–ô–¢–ò: —Ñ–æ—Ä–º—ã –≥–ª–∞–≥–æ–ª–∞"),

            # –†–∞–∑–Ω—ã–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è (–ù–ï –¥–æ–ª–∂–Ω—ã –Ω–∞–π—Ç–∏—Å—å)
            ("—Å–¥–∞—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É", "—Å–Ω—è—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É", "housing", "temporary", 15000, 15000, 30, 30, "‚ùå –ù–ï –î–û–õ–ñ–ï–ù –ù–ê–ô–¢–ò: —Ä–∞–∑–Ω—ã–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è"),
        ]

        results = []
        for text1, text2, category, ex_type, price1, price2, days1, days2, expected in test_cases:
            print(f"\nüìã –¢–µ—Å—Ç: '{text1}' ‚Üî '{text2}'")

            lang_score = self.normalizer.similarity_score(text1, text2)
            print(f"  Language similarity: {lang_score:.3f}")

            # –î–ª—è –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º permanent (—É–ø—Ä–æ—â–∞–µ–º)
            equiv_result = self.equivalence_engine.calculate_permanent_score(price1, price2)
            combined_score = equiv_result.score * 0.7 + lang_score * 0.3

            is_match = equiv_result.is_match and combined_score >= 0.70
            should_match = "‚úÖ –î–û–õ–ñ–ï–ù" in expected

            if should_match and is_match:
                status = "‚úÖ PASS"
            elif not should_match and not is_match:
                status = "‚úÖ PASS"
            else:
                status = "‚ùå FAIL"

            print(f"  Result: {status} | Expected: {expected}")
            print(f"  Combined score: {combined_score:.3f}")

            results.append({
                'test': f"{text1} ‚Üî {text2}",
                'lang_score': lang_score,
                'combined_score': combined_score,
                'is_match': is_match,
                'expected_match': should_match,
                'status': status
            })

        passed = sum(1 for r in results if r['status'] == "‚úÖ PASS")
        total = len(results)
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏/–æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏: {passed}/{total} ({passed/total*100:.1f}%)")

        return results

    def test_category_compatibility(self):
        """1.3 –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å (–º–µ–∂–∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–π –æ–±–º–µ–Ω)"""
        print("\n" + "="*80)
        print("üîÑ 1.3 –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–ê–Ø –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–¨")
        print("="*80)

        # –ú–µ–∂–∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–µ —Ç–µ—Å—Ç—ã - —Ä–∞–∑–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –Ω–æ –æ–¥–∏–Ω —Ç–∏–ø –æ–±–º–µ–Ω–∞
        test_cases = [
            ("–≥–∏—Ç–∞—Ä–∞", "–∫—É—Ä—Å—ã –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ", "music", "education", "temporary", 25000, 15000, 10, 20, "‚úÖ –î–û–õ–ñ–ï–ù –ù–ê–ô–¢–ò: —Ä–∞–∑–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –Ω–æ —É—Å–ª—É–≥–∏"),
            ("–∫–≤–∞—Ä—Ç–∏—Ä–∞ –Ω–∞ —Å—É—Ç–∫–∏", "–∞—Ä–µ–Ω–¥–∞ –∞–≤—Ç–æ", "housing", "transport", "temporary", 20000, 8000, 1, 1, "‚úÖ –î–û–õ–ñ–ï–ù –ù–ê–ô–¢–ò: –∞—Ä–µ–Ω–¥–∞ ‚Üî –∞—Ä–µ–Ω–¥–∞"),
            ("—Ä–∞–±–æ—Ç–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞", "—É—Å–ª—É–≥–∏ –¥–∏–∑–∞–π–Ω–µ—Ä–∞", "services", "services", "temporary", 50000, 30000, 30, 15, "‚úÖ –î–û–õ–ñ–ï–ù –ù–ê–ô–¢–ò: —É—Å–ª—É–≥–∏ ‚Üî —É—Å–ª—É–≥–∏"),
        ]

        results = []
        for want_text, offer_text, want_cat, offer_cat, ex_type, want_price, offer_price, want_days, offer_days, expected in test_cases:
            print(f"\nüìã –¢–µ—Å—Ç: '{want_text}' ({want_cat}) ‚Üî '{offer_text}' ({offer_cat})")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º language similarity
            lang_score = self.normalizer.similarity_score(want_text, offer_text)
            print(f"  Language similarity: {lang_score:.3f}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º equivalence (temporary exchange)
            equiv_result = self.equivalence_engine.calculate_temporary_score(
                want_price, want_days, offer_price, offer_days
            )

            combined_score = equiv_result.score * 0.7 + lang_score * 0.3

            print(f"  Equivalence score: {equiv_result.score:.3f}")
            print(f"  Combined score: {combined_score:.3f}")

            # –í –º–µ–∂–∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–æ–º –æ–±–º–µ–Ω–µ –ª–æ–≥–∏–∫–∞ —Ç–∞ –∂–µ - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            is_match = equiv_result.is_match and combined_score >= 0.70

            # –î–ª—è –º–µ–∂–∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã—Ö –æ–±–º–µ–Ω–æ–≤ –æ—Ü–µ–Ω–∏–≤–∞–µ–º –ø–æ —Å–º—ã—Å–ª—É
            should_match = "‚úÖ –î–û–õ–ñ–ï–ù" in expected

            if should_match and is_match:
                status = "‚úÖ PASS"
            elif not should_match and not is_match:
                status = "‚úÖ PASS"
            else:
                status = "‚ùå FAIL"

            print(f"  Result: {status} | Expected: {expected}")

            results.append({
                'test': f"{want_text} ({want_cat}) ‚Üî {offer_text} ({offer_cat})",
                'lang_score': lang_score,
                'equiv_score': equiv_result.score,
                'combined_score': combined_score,
                'is_match': is_match,
                'expected_match': should_match,
                'status': status
            })

        passed = sum(1 for r in results if r['status'] == "‚úÖ PASS")
        total = len(results)
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: {passed}/{total} ({passed/total*100:.1f}%)")

        return results

    def run_all_tests(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ —Ç–µ—Å—Ç—ã"""
        print("üß™ –ó–ê–ü–£–°–ö –ö–û–ú–ü–õ–ï–ö–°–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –°–ò–°–¢–ï–ú–´ –ü–û–î–ë–û–†–ê –û–ë–ú–ï–ù–û–í")
        print("="*80)

        all_results = []

        # 1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
        semantic_results = self.test_semantic_accuracy()
        all_results.extend(semantic_results)

        # 2. –ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –∏ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—è
        grammar_results = self.test_grammar_orthography()
        all_results.extend(grammar_results)

        # 3. –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
        category_results = self.test_category_compatibility()
        all_results.extend(category_results)

        # –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print("\n" + "="*80)
        print("üìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
        print("="*80)

        total_passed = sum(1 for r in all_results if r['status'] == "‚úÖ PASS")
        total_tests = len(all_results)

        print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤: {total_tests}")
        print(f"–ü—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ: {total_passed}")
        print(f"–£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {total_passed/total_tests*100:.1f}%")

        # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Ç–∏–ø–∞–º
        semantic_passed = sum(1 for r in semantic_results if r['status'] == "‚úÖ PASS")
        grammar_passed = sum(1 for r in grammar_results if r['status'] == "‚úÖ PASS")
        category_passed = sum(1 for r in category_results if r['status'] == "‚úÖ PASS")

        print("\n–ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        print(f"  –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {semantic_passed}/{len(semantic_results)}")
        print(f"  –ì—Ä–∞–º–º–∞—Ç–∏–∫–∞/–æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—è: {grammar_passed}/{len(grammar_results)}")
        print(f"  –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å: {category_passed}/{len(category_results)}")

        return all_results


if __name__ == "__main__":
    tester = MatchingComprehensionTester()
    results = tester.run_all_tests()
